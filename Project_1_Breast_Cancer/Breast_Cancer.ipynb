{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING THE DATASET\n",
    "dataset = pd.read_csv('breast_cancer.csv')\n",
    "X = dataset.iloc[:,1:-1].values\n",
    "y= dataset.iloc[:,-1].values\n",
    "y=np.where(y==2,0,1) # 2 is malignant and 4 is benign\n",
    "\n",
    "#FOR PLOTTING THE DATA\n",
    "models = []\n",
    "accuracies = []\n",
    "std_devs = []\n",
    "\n",
    "sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE DATASET INTO TRAINING SET AND TEST SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING LOGISTIC REGRESSION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_lr = LogisticRegression(random_state=0)\n",
    "classifier_lr.fit(X_train, y_train)\n",
    "\n",
    "#k-fold cross validation\n",
    "accuracies_lr = cross_val_score(estimator = classifier_lr, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "#DATA GATHERING FOR PLOTTING\n",
    "models.append('Logistic Regression')\n",
    "accuracies.append(accuracies_lr.mean() * 100)\n",
    "std_devs.append(accuracies_lr.std() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING K-NEAREST NEIGHBOURS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier_knn.fit(X_train, y_train)\n",
    "\n",
    "#k-fold cross validation\n",
    "accuracies_knn = cross_val_score(estimator = classifier_knn, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "#DATA GATHERING FOR PLOTTING\n",
    "models.append('K-Nearest Neighbours')\n",
    "accuracies.append(accuracies_knn.mean() * 100)\n",
    "std_devs.append(accuracies_knn.std() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING SUPPORT VECTOR MACHINE\n",
    "from sklearn.svm import SVC\n",
    "classifier_svm = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier_svm.fit(X_train, y_train)\n",
    "\n",
    "#k-fold cross validation\n",
    "accuracies_svm = cross_val_score(estimator = classifier_svm, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "#DATA GATHERING FOR PLOTTING\n",
    "models.append('Support Vector Machine')\n",
    "accuracies.append(accuracies_svm.mean() * 100)\n",
    "std_devs.append(accuracies_svm.std() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING KERNEL SUPPORT VECTOR MACHINE\n",
    "from sklearn.svm import SVC\n",
    "classifier_ksvm = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_ksvm.fit(X_train, y_train)\n",
    "\n",
    "#k-fold cross validation\n",
    "accuracies_ksvm = cross_val_score(estimator = classifier_ksvm, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "#DATA GATHERING FOR PLOTTING\n",
    "models.append('Kernel Support Vector Machine')\n",
    "accuracies.append(accuracies_ksvm.mean() * 100)\n",
    "std_devs.append(accuracies_ksvm.std() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING RANDOM FOREST CLASSIFIER\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier_rf.fit(X_train, y_train)\n",
    "\n",
    "#k-fold cross validation\n",
    "accuracies_rf = cross_val_score(estimator = classifier_rf, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "#DATA GATHERING FOR PLOTTING\n",
    "models.append('Random Forest Classifier')\n",
    "accuracies.append(accuracies_rf.mean() * 100)\n",
    "std_devs.append(accuracies_rf.std() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING NAIVE BAYES\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_nb = GaussianNB()\n",
    "classifier_nb.fit(X_train, y_train)\n",
    "\n",
    "#k-fold cross validation\n",
    "accuracies_nb = cross_val_score(estimator = classifier_nb, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "#DATA GATHERING FOR PLOTTING\n",
    "models.append('Naive Bayes')\n",
    "accuracies.append(accuracies_nb.mean() * 100)\n",
    "std_devs.append(accuracies_nb.std() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING DECISION TREE CLASSIFIER\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier_dt.fit(X_train, y_train)\n",
    "\n",
    "#k-fold cross validation\n",
    "accuracies_dt = cross_val_score(estimator = classifier_dt, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "#DATA GATHERING FOR PLOTTING\n",
    "models.append('Decision Tree Classifier')\n",
    "accuracies.append(accuracies_dt.mean() * 100)\n",
    "std_devs.append(accuracies_dt.std() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING XGBOOST \n",
    "from xgboost import XGBClassifier\n",
    "classifier_xg = XGBClassifier(objective='binary:logistic')\n",
    "classifier_xg.fit(X_train, y_train)\n",
    "\n",
    "#k-fold cross validation\n",
    "accuracies_xg = cross_val_score(estimator = classifier_xg, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "#DATA GATHERING FOR PLOTTING\n",
    "models.append('XGBoost')\n",
    "accuracies.append(accuracies_xg.mean() * 100)\n",
    "std_devs.append(accuracies_xg.std() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING CATBOOST\n",
    "from catboost import CatBoostClassifier\n",
    "classifier_cat = CatBoostClassifier()\n",
    "classifier_cat.fit(X_train, y_train)\n",
    "\n",
    "#k-fold cross validation\n",
    "accuracies_cat = cross_val_score(estimator = classifier_cat, X = X_train, y = y_train, cv = 10)\n",
    "\n",
    "#DATA GATHERING FOR PLOTTING\n",
    "models.append('CatBoost')\n",
    "accuracies.append(accuracies_cat.mean() * 100)\n",
    "std_devs.append(accuracies_cat.std() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINTING ACCURACY AND STANDARD DEVIATION\n",
    "print(\"Accuracy with Logistic Regression: {:.2f} %\".format(accuracies_lr.mean()*100))\n",
    "print(\"Standard Deviation with Logistic Regression: {:.2f} %\".format(accuracies_lr.std()*100))\n",
    "print(\"___________________________________________________\")\n",
    "print(\"Accuracy with K-Nearest Neighbours: {:.2f} %\".format(accuracies_knn.mean()*100))\n",
    "print(\"Standard Deviation with K-Nearest Neighbours: {:.2f} %\".format(accuracies_knn.std()*100))\n",
    "print(\"___________________________________________________\")\n",
    "print(\"Accuracy with Support Vector Machine: {:.2f} %\".format(accuracies_svm.mean()*100))\n",
    "print(\"Standard Deviation with Support Vector Machine: {:.2f} %\".format(accuracies_svm.std()*100))\n",
    "print(\"___________________________________________________\")\n",
    "print(\"Accuracy with Kernel Support Vector Machine: {:.2f} %\".format(accuracies_ksvm.mean()*100))\n",
    "print(\"Standard Deviation with Kernel Support Vector Machine: {:.2f} %\".format(accuracies_ksvm.std()*100))\n",
    "print(\"___________________________________________________\")\n",
    "print(\"Accuracy with Randon Forest: {:.2f} %\".format(accuracies_rf.mean()*100))\n",
    "print(\"Standard Deviation with Random Forest: {:.2f} %\".format(accuracies_rf.std()*100))\n",
    "print(\"___________________________________________________\")\n",
    "print(\"Accuracy with Naive Bayes: {:.2f} %\".format(accuracies_nb.mean()*100))\n",
    "print(\"Standard Deviation with Naive Bayes: {:.2f} %\".format(accuracies_nb.std()*100))\n",
    "print(\"___________________________________________________\")\n",
    "print(\"Accuracy with Decision Tree: {:.2f} %\".format(accuracies_dt.mean()*100))\n",
    "print(\"Standard Deviation with Decision Tree: {:.2f} %\".format(accuracies_dt.std()*100))\n",
    "print(\"___________________________________________________\")\n",
    "print(\"Accuracy with XGBoost: {:.2f} %\".format(accuracies_xg.mean()*100))\n",
    "print(\"Standard Deviation with XGBoost: {:.2f} %\".format(accuracies_xg.std()*100))\n",
    "print(\"___________________________________________________\")\n",
    "print(\"Accuracy with CatBoost: {:.2f} %\".format(accuracies_cat.mean()*100))\n",
    "print(\"Standard Deviation with CatBoost: {:.2f} %\".format(accuracies_cat.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING THE ACCURACY AND STANDARD DEVIATION\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure models, accuracies, and std_devs are already populated\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(models, accuracies, yerr=std_devs, capsize=7, color='red', edgecolor='black')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy Comparison (with Standard Deviation)')\n",
    "plt.ylim([min(accuracies) - 5, 100])\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add accuracy and std deviation labels\n",
    "for bar, acc, std in zip(bars, accuracies, std_devs):\n",
    "    bar_x = bar.get_x() + bar.get_width() / 2.0\n",
    "    bar_height = bar.get_height()\n",
    "\n",
    "    # Accuracy label just above the bar\n",
    "    plt.text(bar_x, bar_height + 0.3, f'{acc:.2f}%', ha='center', va='bottom', fontsize=9, color='black')\n",
    "    \n",
    "    # Standard deviation label above the error bar\n",
    "    plt.text(bar_x, bar_height + std + 0.3, f'±{std:.2f}', ha='center', va='bottom', fontsize=8, color='blue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOING FORWARD WITH KNN MODEL AS IT HAS BEST ACCURACY\n",
    "##APPLYING K-FOLDS CROSS VALIDATION & GRID SEARCH\n",
    "\n",
    "print(\"K-Fold validation Accuracy with K-Nearest Neighbours: {:.2f} %\".format(accuracies_knn.mean()*100))\n",
    "print(\"K-Fold validation Standard Deviation with K-Nearest Neighbours: {:.2f} %\".format(accuracies_knn.std()*100))\n",
    "print(\"______________________________________________\")\n",
    "\n",
    "#Applying Grid Search for Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'n_neighbors': list(range(3, 21, 2)),          # Try odd values between 3 and 20\n",
    "    'weights': ['uniform', 'distance'],            # 'uniform' = equal weight, 'distance' = closer neighbors weigh more\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Distance metric\n",
    "    'p': [1, 2]                                     # For Minkowski: 1 = manhattan, 2 = euclidean\n",
    "}\n",
    "grid_search = GridSearchCV(estimator = classifier_knn,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"______________________________________________\")\n",
    "print(\"Best Accuracy after Grid Search: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters after Grid Search:\", best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING KNN WITH BEST PARAMETERS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_knn_updated = KNeighborsClassifier(n_neighbors = 9, metric = 'euclidean', p = 1, weights='uniform')\n",
    "classifier_knn_updated.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX AND ACCURACY SCORE\n",
    "y_pred = classifier_knn_updated.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTING A NEW RESULT\n",
    "classifier_knn_updated.predict([[8,7,5,10,7,9,5,5,4]])\n",
    "if(classifier_knn_updated.predict([[8,7,5,10,7,9,5,5,4]])==1):\n",
    "    print(\"The Tumor is Benign\")\n",
    "else:\n",
    "    print(\"The Tumor is Malignant\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
