{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARING DATASET & PRINTING\n",
    "dataset = pd.read_csv('Car_Purchasing_Data.csv', encoding = 'ISO-8859-1')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZING THE DATASET\n",
    "sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING THE DATASET BY DROPPING UNNECESSARY COLUMNS\n",
    "#Unecessary columns = 'Customer Name', 'Customer Email' and dropping 'Car Purchase Amount' too as it is the dependent variable\n",
    "\n",
    "X = dataset.drop(['Customer Name', 'Customer e-mail', 'Car Purchase Amount'], axis = 1)\n",
    "y = dataset['Car Purchase Amount']\n",
    "\n",
    "#VISUALIZING THE DATASET AFTER DROPPING UNNECESSARY COLUMNS\n",
    "#X\n",
    "#y\n",
    "\n",
    "#CHECKING SHAPE OF X and y\n",
    "#X.shape\n",
    "#y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE HOT ENCODING CATEGORICAL DATA - 'County' column\n",
    "#Post One hot encoding the number of columns have inclreased from 6 to 216 due to different countries\n",
    "# Here, 'Country' means the actual column name\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(sparse_output=False), ['Country'])], remainder='passthrough')\n",
    "X = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLYING FEATURE SCALING TO NORMALIZE THE VALUES\n",
    "sc_x = MinMaxScaler()\n",
    "sc_y = MinMaxScaler()\n",
    "X = sc_x.fit_transform(X)\n",
    "y = sc_y.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZING THE DATASET AFTER FEATURE SCALING\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE DATASET INTO TRAINING AND TESTING SETS\n",
    "#Here, 80% of the data is used for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILDING THE ARTIFICIAL NEURAL NETWORK(ANN) MODEL \n",
    "#INITIALIZING THE ANN\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING THE INPUT LAYER AND FIRST HIDDEN LAYER\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu', input_dim=216))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING THE SECOND HIDDEN LAYER\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=16, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING THE THIRD HIDDEN LAYER\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING THE FOURTH HIDDEN LAYER\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING THE OUTPUT LAYER\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING THE MODEL SUMMARY\n",
    "\n",
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPILING THE ANN\n",
    "\n",
    "ann.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE ANN ON THE TRAINING SET\n",
    "\n",
    "output = ann.fit(X_train, y_train, batch_size=5, epochs=500, verbose=1, validation_split=0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATING THE MODEL\n",
    "#These are the values that are returned by the fit method after every epoch\n",
    "\n",
    "output.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING ALL THE VALUES RETURNED BY THE FIT METHOD\n",
    "\n",
    "plt.plot(output.history['loss'])\n",
    "plt.plot(output.history['val_loss'])\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATING THE MEAN ABSOLUTE ERROR AND R2 VALUE\n",
    "\n",
    "#Predicting values on full test set\n",
    "y_pred = ann.predict(X_test)\n",
    "\n",
    "# Inverse scale if needed\n",
    "y_pred_original = sc_y.inverse_transform(y_pred.reshape(-1, 1))\n",
    "y_test_original = sc_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Then evaluate\n",
    "# Calculate MAE and R² score\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R² Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTING NEW VALUE\n",
    "\n",
    "#Preparing the raw input data\n",
    "5\n",
    "\n",
    "#Encodinig the Country column using the same ColumnTransformer (ct) you used during training\n",
    "X_new_encoded = ct.transform(X_new)\n",
    "\n",
    "# Applying the same Scaler you used during training\n",
    "X_new_scaled = sc_x.transform(X_new_encoded)\n",
    "\n",
    "# Predicting the scaled value\n",
    "y_pred_new = ann.predict(X_new_scaled)\n",
    "\n",
    "\n",
    "#y_pred value is scaled, so we need to inverse transform it to get the original value\n",
    "#We use the same scaler (sc_y) you used during training to inverse transform the predicted value\n",
    "y_pred_original = sc_y.inverse_transform(y_pred_new.reshape(-1, 1))\n",
    "\n",
    "print(f\"Expected Purchase Amount: {y_pred_original[0][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
